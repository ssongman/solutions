# Traffic 감시



# 1. 개요

Kubernetes 환경에서 Pod에서 외부로 나가는 트래픽의 내용을 관찰하고 **Source IP**와 **Target IP**를 확인한다.



# 2. Traffic 감시하는 방법 검토

## **1. tcpdump 또는 Wireshark 사용**



**방법**

1.	tcpdump**를 Pod에 설치하여 네트워크 트래픽 캡처**:

•	Pod에서 tcpdump를 실행하면 네트워크 트래픽을 실시간으로 캡처할 수 있습니다.

2.	**실행 방법**:

•	Pod 내부에 tcpdump 설치:

```sh

kubectl exec -it <pod-name> -- apk add --no-cache tcpdump

```



•	트래픽 캡처 실행:

```sh

kubectl exec -it <pod-name> -- tcpdump -n -i eth0

```

•	-n: 호스트 이름 대신 IP 주소를 표시.

•	-i eth0: Pod의 기본 네트워크 인터페이스(다른 인터페이스 이름일 수 있음).

•	특정 포트나 IP를 필터링하려면:

```sh

tcpdump -n host <target-ip> or port <port-number>

```





3. **트래픽 분석**:

•	캡처된 데이터를 파일로 저장:

```sh

tcpdump -n -i eth0 -w /tmp/traffic.pcap

```



•	파일을 다운로드하여 Wireshark로 분석:

```sh

kubectl cp <namespace>/<pod-name>:/tmp/traffic.pcap ./traffic.pcap

```



## **2. Kubernetes Network Policies와 Cilium 사용**



**방법**

1.	**Cilium**은 Kubernetes 네트워크 플러그인(CNI)으로, 트래픽 흐름을 상세히 관찰할 수 있는 기능을 제공합니다.

2.	**설치 후 트래픽 확인**:

•	**Hubble**을 통해 Cilium 트래픽을 시각화:

```sh

$ hubble observe --from-pod <namespace>/<pod-name>

```

•	--from-pod 옵션으로 특정 Pod에서 나가는 트래픽만 캡처.



3.	**출력 내용**:

•	Source IP, Destination IP, Protocol, Port 정보 확인 가능.





# 3. 실습1



## 1) 예제POD

`client` Pod를 만듭니다. 다음 명령은 `client` Pod에서 Bash를 실행합니다.

```sh
$ kubectl run curl -n demo \
    --image=curlimages/curl \
    --labels="app=client" \ 
    --command -- sleep 365d

```



```sh
$ kubectl -n demo exec -it curl -- sh


# 성공
$ curl https://google.com -i -k -m 3
HTTP/2 301
location: https://www.google.com/

# 성공
$ curl -i -k https://github.com


# naver.com 은 막혀 있어서 나가지 않음
$ curl https://naver.com


```

허용된 도메인은 정상적으로 응답을 받고, 차단된 도메인은 연결이 실패한다.





```sh
$ hubble observe --from-pod demo/curl --follow

```







```sh
$ curl https://google.com -i -k -m 3
HTTP/2 301
location: https://www.google.com/


# hubble observe
Dec 22 14:16:41.707: demo/curl:41618 (ID:4306) -> kube-system/coredns-54b69f46b8-gdjkx:53 (ID:13220) to-overlay FORWARDED (UDP)
Dec 22 14:16:41.709: demo/curl:58121 (ID:4306) -> kube-system/coredns-54b69f46b8-gdjkx:53 (ID:13220) to-overlay FORWARDED (UDP)
Dec 22 14:16:41.711: demo/curl:52389 (ID:4306) -> kube-system/coredns-54b69f46b8-742hx:53 (ID:13220) to-overlay FORWARDED (UDP)
Dec 22 14:16:41.712: demo/curl:50790 (ID:4306) -> kube-system/coredns-54b69f46b8-742hx:53 (ID:13220) to-overlay FORWARDED (UDP)
Dec 22 14:16:41.714: demo/curl:52097 (ID:4306) -> kube-system/coredns-54b69f46b8-gdjkx:53 (ID:13220) to-overlay FORWARDED (UDP)
Dec 22 14:16:41.718: demo/curl:42098 (ID:4306) -> 172.217.25.174:443 (world) to-stack FORWARDED (TCP Flags: SYN)
Dec 22 14:16:41.734: demo/curl:42098 (ID:4306) -> 172.217.25.174:443 (world) to-stack FORWARDED (TCP Flags: ACK)
Dec 22 14:16:41.737: demo/curl:42098 (ID:4306) -> 172.217.25.174:443 (world) to-stack FORWARDED (TCP Flags: ACK, PSH)
Dec 22 14:16:41.864: demo/curl:42098 (ID:4306) -> 172.217.25.174:443 (world) to-stack FORWARDED (TCP Flags: ACK, FIN)
Dec 22 14:16:41.879: demo/curl:42098 (ID:4306) -> 172.217.25.174:443 (world) to-stack FORWARDED (TCP Flags: ACK)

```





# 4. 실습2

두 개의 Kubernetes 클러스터 간 트래픽을 생성하여 외부에서 네트워크 트래픽을 관찰하여 **Source IP**와 **Target IP**를 확인한다.



## 1) 개요

### (1) 시나리오



1.	**Source IP**와 **Target IP**를 확인.

2.	첫 번째 클러스터에서 두 번째 클러스터로 트래픽을 생성.

3.	두 번째 클러스터에서 트래픽을 캡처하고 분석.



### (2) **구체적인 설정 및 방법**



**1. 클러스터 A → 클러스터 B**



​	•	클러스터 A는 트래픽을 생성하는 역할.

​	•	클러스터 B는 트래픽을 수신하고 관찰.







## 2) AKS 설정



### (1) AKS BYOCNI cluster 생성

```sh

RESOURCE_GROUP_NAME=yj-rg
CLUSTER_NAME=yj-aks11


# Create AKS cluster
# 반드시 network-plugin 을 none 으로 생성
$ az aks create \
    --location koreacentral \
    --resource-group $RESOURCE_GROUP_NAME \
    --name $CLUSTER_NAME \
    --network-plugin none \
    --generate-ssh-keys


# credentials
$ az aks get-credentials -n yj-aks -g yj-rg
$ az aks get-credentials -n yj-aks2 -g yj-rg
$ az aks get-credentials -n yj-aks11 -g yj-rg

```



### (2) cilium 설치

```sh

$ helm repo add cilium https://helm.cilium.io/


# 아래 helm install 명령연은 AKS BYOCNI cluster
$ helm install cilium cilium/cilium --version 1.16.4 \
  --namespace kube-system \
  --set aksbyocni.enabled=true \
  --set nodeinit.enabled=true


```





## 3) 클러스터 A에서 트래픽 생성



### 1. 트래픽 생성 Pod 배포

클러스터 A에 curl 또는 wget 명령이 포함된 간단한 Pod를 배포합니다.



```sh
$ kubectl run curl -n demo \
    --image=curlimages/curl \
    --labels="app=client" \ 
    --command -- sleep 365d


$ curl http://<external-ip-of-cluster-b-service>:80

$ curl http://20.249.197.5:80

```



•	<external-ip-of-cluster-b-service>는 클러스터 B의 Service 외부 IP입니다.





## 4) 클러스터 B에서 트래픽 수신 및 관찰



### 1. 수신 서비스 및 Pod 배포

클러스터 B에서 HTTP 요청을 처리하고 트래픽을 캡처할 Pod를 배포합니다.

```sh

$ kubectl create ns demo


$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: v1
kind: Service
metadata:
  name: traffic-receiver
spec:
  selector:
    app: traffic-receiver
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traffic-receiver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traffic-receiver
  template:
    metadata:
      labels:
        app: traffic-receiver
    spec:
      containers:
      - name: traffic-receiver
        image: nginx
        ports:
        - containerPort: 80
EOF


```

•	Service의 외부 IP를 클러스터 A의 트래픽 생성 Pod에서 사용.



### 2. tcpdump Pod 배포

수신 Pod와 동일한 노드에서 실행되는 Pod에 tcpdump를 설치합니다.

```sh


$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: tcpdump
spec:
  hostNetwork: true
  nodeName: aks-nodepool1-43823302-vmss000000
  containers:
  - name: tcpdump
    image: nicolaka/netshoot
    command:
    - /bin/sh
    - -c
    - "tcpdump -i eth0"
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
        - NET_RAW
EOF





$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: tcpdump
spec:
  hostNetwork: true
  nodeName: aks-nodepool1-97568771-vmss000001
  containers:
  - name: tcpdump
    image: nicolaka/netshoot
    command:
    - /bin/sh
    - -c
    - "tcpdump -i eth0"
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
        - NET_RAW
EOF



```





### 3. tcpdump 실행



```sh

# pod 내로 진입
$ kubectl exec -it tcpdump bash


$ tcpdump -n host <pod-ip-of-traffic-receiver>

$ tcpdump -n host 20.249.197.5



```

•	<pod-ip-of-traffic-receiver>는 traffic-receiver Pod의 IP입니다.

•	트래픽 캡처에서 Source IP와 Target IP 확인.









## 5) Source IP와 Target IP 분석



1.	**Source IP**:

•	클러스터 A에서 오는 요청의 원본 IP.

•	만약 클러스터 A의 Pod IP가 아닌 노드 IP로 표시되면, 이는 NAT(Network Address Translation) 때문.

2.	**Target IP**:

•	클러스터 B의 Service가 외부에 노출한 LoadBalancer IP 또는 Pod IP.





```

< 소스 > 
pod IP : 10.0.1.247
Node IP : 10.224.0.4


< target > 
20.249.197.5


<tcpdump >
tcpdump -n host 20.249.197.5

tcpdump -n host 10.0.1.247

tcpdump -n host 10.224.0.4



14:49:38.494441 IP 10.224.0.6.50362 > 10.224.0.4.10250: Flags [.], ack 137597, win 501, options [nop,nop,TS val 1887235677 ecr 515180001], length 0
14:49:38.504023 IP 20.214.101.250.443 > 10.224.0.4.35172: Flags [P.], seq 1496:1579, ack 13836, win 2390, options [nop,nop,TS val 1110369339 ecr 2036373635], length 83
14:49:38.504313 IP 10.224.0.4.35172 > 20.214.101.250.443: Flags [P.], seq 13836:13888, ack 1579, win 501, options [nop,nop,TS val 2036374609 ecr 1110369339], length 52
14:49:38.504328 IP 10.224.0.4.57728 > 10.224.0.6.10250: Flags [P.], seq 117:156, ack 277, win 501, options [nop,nop,TS val 515180011 ecr 1887232692], length 39
14:49:38.504800 IP 20.214.101.250.443 > 10.224.0.4.35172: Flags [.], ack 13888, win 2390, options [nop,nop,TS val 1110369340 ecr 2036374609], length 0
14:49:38.504903 IP 20.214.101.250.443 > 10.224.0.4.35

```











## 추가: NAT 없이 Source IP 유지하기



클러스터 B에서 Service의 externalTrafficPolicy를 Local로 설정하면, NAT 없이 실제 Source IP를 유지할 수 있습니다.

1.	**Service 수정**:

```sh

spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  
```



2.	**결과**:

•	클러스터 A의 Pod IP가 클러스터 B에서 트래픽의 Source IP로 확인.









# 5. Cilium EgressGateway



https://tech.kakaopay.com/post/cilium-egress-gateway/







```sh

helm fetch cilium/cilium --version 1.16.4



```





### 1) Cilium Egress Gateway 활성화

```sh



# 아래 helm install 명령은 AKS BYOCNI cluster
$ helm -n kube-system upgrade --install cilium cilium/cilium --version 1.16.4 \
  --set aksbyocni.enabled=true \
  --set nodeinit.enabled=true \
  --set egressGateway.enabled=true


# -------------------------------
  --set egressGateway.installRoutes=false \
  --set egressGateway.reconciliationTriggerInterval=1s \
  --set bpf.masquerade=true \
  --set kubeProxyReplacement=true
# -------------------------------


$ helm -n kube-system ls
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
cilium  kube-system     2               2024-12-22 22:48:30.369355 +0900 KST    deployed        cilium-1.16.4   1.16.4
cilium  kube-system     3               2024-12-23 00:57:34.383605 +0900 KST    deployed        cilium-1.16.4   1.16.4



$ helm -n kube-system get values cilium

$ helm -n kube-system get values cilium --revision 3
USER-SUPPLIED VALUES:
aksbyocni:
  enabled: true
egressGateway:
  enabled: true
nodeinit:
  enabled: true



```

- egressGateway.enabled egressGateway를 사용하기 위해 true로 설정



#### helm 으로 설치가 안된다. ㅠㅠ

```sh


# 최신 Cilium 버전의 CRD를 다운로드:
curl -L https://raw.githubusercontent.com/cilium/cilium/v1.16.4/install/kubernetes/quick-install.yaml -o cilium-quick-install.yaml


1.16.4


kubectl apply -f cilium-quick-install.yaml


kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.16.4/install/kubernetes/quick-install.yaml

------------------------

curl -L https://raw.githubusercontent.com/cilium/cilium/v1.16.0/install/kubernetes/cilium-crd.yaml -o cilium-crd.yaml



```





#### 확인

```sh


$  kubectl get crds | grep egress

ciliumnetworkpolicies.cilium.io
ciliumegressgatewaypolicies.cilium.io


```









### 2) Container 환경에 IP address 추가

```sh

$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "egress-ip-assign"
  labels:
    name: "egress-ip-assign"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "egress-ip-assign"
  template:
    metadata:
      labels:
        name: "egress-ip-assign"
    spec:
      nodeName: aks-nodepool1-97568771-vmss000001
      hostNetwork: true
      containers:
      - name: egress-ip
        image: docker.io/library/busybox:1.31.1
        command: ["/bin/sh","-c"]
        securityContext:
          privileged: true
        args:
        - "ip address add 10.224.0.51/24 dev eth0;"
        lifecycle:
          preStop:
            exec:
              command:
              - "/bin/sh"
              - "-c"
              - "ip address del 10.224.0.51/24 dev eth0;"
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
            - NET_RAW
EOF



# 삭제시...
$ kubectl -n demo delete deploy egress-ip-assign


```

* lifecycle
  * Kubernetes에서 컨테이너의 **라이프사이클 이벤트**에 대해 특정 작업을 정의하는 데 사용된다. 
  * 이 섹션은 컨테이너가 시작되거나 종료되기 전에 실행해야 하는 명령을 지정할 수 있다.
  * preStop: **컨테이너가 종료되기 직전에** 실행될 명령을 정의
  * 컨테이너가 종료될 때 eth0 인터페이스에서 특정 IP 주소를 제거하여 **리소스를 정리**하거나 **충돌을 방지**하기 위함



### 3) CiliumEgressGatewayPolicy

```sh

$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: cilium.io/v2
kind: CiliumEgressGatewayPolicy
metadata:
  name: proxy-egress-policy
spec:
  # Specify which pods should be subject to the current policy.
  # Multiple pod selectors can be specified.
  selectors:
    - podSelector:
        matchLabels:
          app: client1
  # Specify which destination CIDR(s) this policy applies to.
  # Multiple CIDRs can be specified.
  destinationCIDRs:
    - "0.0.0.0/0"

  # Configure the gateway node.
  egressGateway:
    # Specify which node should act as gateway for this policy.
    nodeSelector:
      matchLabels:
        kubernetes.io/hostname: aks-nodepool1-97568771-vmss000001
    egressIP: 10.224.0.51
---
apiVersion: cilium.io/v2
kind: CiliumEgressGatewayPolicy
metadata:
  name: fep-egress-policy
spec:
  # Specify which pods should be subject to the current policy.
  # Multiple pod selectors can be specified.
  selectors:
    - podSelector:
        matchLabels:
          app: client2

  # Specify which destination CIDR(s) this policy applies to.
  # Multiple CIDRs can be specified.
  destinationCIDRs:
    - "0.0.0.0/0"

  # Configure the gateway node.
  egressGateway:
    # Specify which node should act as gateway for this policy.
    nodeSelector:
      matchLabels:
        kubernetes.io/hostname: aks-nodepool1-97568771-vmss000001
    egressIP: 10.224.0.52
EOF


# 삭제시...
$ kubectl -n demo delete CiliumEgressGatewayPolicy proxy-egress-policy
  kubectl -n demo delete CiliumEgressGatewayPolicy fep-egress-policy


```



#### 참고

```sh


$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: cilium.io/v2
kind: CiliumEgressGatewayPolicy
metadata:
  name: example-egress-gateway-policy
spec:
  egressGateway: kube-system/egress-gateway-node
  endpointSelector:
    matchLabels:
      app: frontend
  destinationCIDRs:
  - 0.0.0.0/0
EOF



# 삭제시...
$ kubectl -n demo delete CiliumEgressGatewayPolicy example-egress-gateway-policy
  
```









### 4) client pod 

```sh

$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl1
  namespace: demo
  labels:
    app: client1
spec:
  nodeName: aks-nodepool1-97568771-vmss000001
  containers:
  - name: curl
    image: curlimages/curl
    command: [ "sleep", "365d" ]
EOF


$ cat <<EOF | kubectl -n demo apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl2
  namespace: demo
  labels:
    app: client2
spec:
  nodeName: aks-nodepool1-97568771-vmss000001
  containers:
  - name: curl
    image: curlimages/curl
    command: [ "sleep", "365d" ]
EOF



# 삭제시....
$ kubectl -n demo delete pod curl1
  kubectl -n demo delete pod curl2


# test
$ curl http://<external-ip-of-cluster-b-service>:80


$ curl http://20.249.197.5:80




```



