# Promtheus Stack





# 1. 개요

Kubernetes 클러스터에서 Grafana를 사용하여 모니터링을 설정하고 관리하는 방법을 가이드한다.



# 2. kube-prometheus-stack 설치

* `kube-prometheus-stack`은 Prometheus, Grafana, 그리고 필요한 메트릭 수집기를 포함하는 Helm 차트임

* kube-prometheus-stack Helm Chart 구성도

  * Prometheus
  * Grafana
  * 메트릭 수집기
    * kube-state-metrics
    * node-exporter

  



## 1) [참고] Helm 설치(설치되지 않은 경우)

```sh
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
```





## 2) prometheus CRD 설치



```sh

# CRD 설치
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml


customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied



## 확인
$ kubectl get crds | grep monitoring.coreos.com
alertmanagers.monitoring.coreos.com                   2024-10-20T05:34:52Z
podmonitors.monitoring.coreos.com                     2024-10-20T05:34:52Z
probes.monitoring.coreos.com                          2024-10-20T05:34:53Z
prometheuses.monitoring.coreos.com                    2024-10-20T05:34:54Z
prometheusrules.monitoring.coreos.com                 2024-10-20T05:34:54Z
servicemonitors.monitoring.coreos.com                 2024-10-20T05:34:55Z
thanosrulers.monitoring.coreos.com                    2024-10-20T05:34:55Z

```





## 3) Helm 저장소 추가 및 업데이트

```sh
$ 
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```





## 4) Helm values.yaml 확인

```sh
$ helm search repo kube-prometheus-stack
NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
prometheus-community/kube-prometheus-stack      62.6.0          v0.76.1         kube-prometheus-stack collects Kubernetes manif...

NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
prometheus-community/kube-prometheus-stack      65.3.1          v0.77.1         kube-prometheus-stack collects Kubernetes manif...


# values 확인
$ helm show values prometheus-community/kube-prometheus-stack


```





## 5) kube-prometheus-stack 설치



```sh
# NS 생성
$ kubectl create ns monitoring

```



### (1) DEV

```sh

# DEV
$ helm -n monitoring upgrade --install prometheus-stack prometheus-community/kube-prometheus-stack \
  --set alertmanager.enabled=false \
  --set grafana.enabled=true \
  --set grafana.adminPassword=New1234! \
  --set grafana.ingress.enabled=true \
  --set grafana.ingress.ingressClassName=nginx \
  --set 'grafana.ingress.hosts[0]'=grafana.dev.abclab.ktds.com \
  --set grafana.persistence.enabled=true \
  --set grafana.persistence.storageClassName=default \
  --set prometheus.ingress.enabled=true \
  --set prometheus.ingress.ingressClassName=nginx \
  --set 'prometheus.ingress.hosts[0]'=prometheus.20.249.129.204.nip.io \
  --set prometheus.prometheusSpec.retention=10d \
  --set prometheus.prometheusSpec.retentionSize=""


# list
$ helm -n monitoring ls
NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
prometheus-stack        monitoring      14              2024-10-10 22:13:37.483909 +0900 KST    deployed        kube-prometheus-stack-65.1.0    v0.77.1


## 작업 History
$ helm -n monitoring history prometheus-stack
REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION
5               Tue Sep 10 14:38:00 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
6               Tue Sep 10 14:42:33 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
7               Tue Sep 10 14:50:42 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
8               Tue Sep 10 14:58:04 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
9               Tue Sep 10 14:58:58 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
10              Tue Sep 10 15:00:33 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
11              Tue Sep 10 15:01:56 2024        superseded      kube-prometheus-stack-62.6.0    v0.76.1         Upgrade complete
12              Thu Oct 10 19:59:07 2024        failed          kube-prometheus-stack-63.0.0    v0.76.1         Upgrade "prometheus-stack" failed: cannot patch "prometheus-stack-kube-prom-prometheus" with kind Prometheus: Prometheus.monitoring.coreos.com "prometheus-stack-kube-prom-prometheus" is invalid: spec.retentionSize: Invalid value: "“”": spec.retentionSize in body should match '(^0|([0-9]*[.])?[0-9]+((K|M|G|T|E|P)i?)?B)$'
13              Thu Oct 10 20:00:34 2024        superseded      kube-prometheus-stack-63.0.0    v0.76.1         Upgrade complete
14              Thu Oct 10 22:13:37 2024        deployed        kube-prometheus-stack-65.1.0    v0.77.1         Upgrade complete

# 특정 revision 별 values.yaml 확인
$ helm -n monitoring get values prometheus-stack --revision 14





#[참고: 기타 옵션 참고사항]-----------------------------------------
  
  --set grafana.persistence.enabled=true \
  --set grafana.persistence.storageClassName=default \
  --set grafana.persistence.size=10Gi \
  
  --set grafana.persistence.storageClassName=azurefile-csi \
  --set prometheus.prometheusSpec.retention=10d \
  --set prometheus.prometheusSpec.retentionSize="" \
  --dry-run=true
#[참고: 기타 옵션 참고사항]-----------------------------------------

# storageClass 가 azurefile-csi가 아닌 default 로 설정해야 함
# azurefile-csi의 경우 storage account 의 file shared 로 잡히는데 권한문제 발생함
# default 로 설정해야 disk(block storage, NFS 처럼) pvc 프로비저닝 됨
  
  
############
NAME: prometheus
LAST DEPLOYED: Mon Sep  9 17:07:17 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.



# 삭제시...
$ helm -n monitoring delete prometheus-stack


```

* storageClass 가 azurefile-csi가 아닌 default 로 설정해야 함
* azurefile-csi의 경우 storage account 의 file shared 로 잡히는데 권한문제 발생함
* default 로 설정해야 disk(block storage, NFS 처럼) pvc 프로비저닝 됨



### (2) PRD

```sh

# PRD
$ helm -n monitoring upgrade --install prometheus-stack prometheus-community/kube-prometheus-stack \
  --set alertmanager.enabled=false \
  --set grafana.enabled=true \
  --set grafana.adminPassword=Prd1234! \
  --set grafana.ingress.enabled=true \
  --set grafana.ingress.ingressClassName=nginx \
  --set 'grafana.ingress.hosts[0]'=grafana.abclab.ktds.com \
  --set 'grafana.ingress.tls[0].secretName'=abc-wildcard-cert \
  --set grafana.persistence.enabled=true \
  --set grafana.persistence.storageClassName=default \
  --set grafana.persistence.size=10Gi \
  --set grafana.nodeSelector.nodepool=infra \
  --set prometheus.ingress.enabled=true \
  --set prometheus.ingress.ingressClassName=nginx \
  --set 'prometheus.ingress.hosts[0]'=prometheus.4.230.144.108.nip.io \
  --set prometheus.prometheusSpec.retention=10d \
  --set prometheus.prometheusSpec.retentionSize="" \
  --set prometheus.prometheusSpec.nodeSelector.nodepool=infra \
  --set kube-state-metrics.nodeSelector.nodepool=infra \
  --set prometheusOperator.nodeSelector.nodepool=infra 


# list
$ helm -n monitoring ls
NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
prometheus-stack        monitoring      4               2024-10-10 22:57:42.077447 +0900 KST    deployed        kube-prometheus-stack-65.1.0    v0.77.1



## 작업 History
$ helm -n monitoring history prometheus-stack
REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION
1               Fri Sep 27 15:13:01 2024        superseded      kube-prometheus-stack-63.0.0    v0.76.1         Install complete
2               Mon Sep 30 18:02:50 2024        deployed        kube-prometheus-stack-63.0.0    v0.76.1         Upgrade complete
3               Thu Oct 10 22:35:14 2024        deployed        kube-prometheus-stack-65.1.0    v0.77.1         Upgrade complete
4               Thu Oct 10 22:57:42 2024        deployed        kube-prometheus-stack-65.1.0    v0.77.1         Upgrade complete


# 특정 revision 별 values.yaml 확인
$ helm -n monitoring get values prometheus-stack --revision 4



# 삭제시...
$ helm -n monitoring delete prometheus-stack


```

* storageClass 가 azurefile-csi가 아닌 default 로 설정해야 함
* azurefile-csi의 경우 storage account 의 file shared 로 잡히는데 권한문제 발생함
* default 로 설정해야 disk(block storage, NFS 처럼) pvc 프로비저닝 됨





## 7) 기타 Helm 명령



#### [참고] 현재 릴리즈에 사용된 전체 values.yaml 추출

```sh
# 현재 릴리즈에 사용된 전체 values.yaml 추출
$ helm -n monitoring get values prometheus-stack --all

$ helm -n monitoring get values prometheus-stack --all > current-values.yaml

```



#### [참고] 특정 revision에서 사용된 values.yaml 확인

```sh

# 특정 revision 에서 사용된 values.yaml 확인
$ helm -n monitoring get values prometheus-stack --revision 13
USER-SUPPLIED VALUES:
alertmanager:
  enabled: false
grafana:
  adminPassword: New1234!
  enabled: true
  ingress:
    enabled: true
    hosts:
    - grafana.20.249.129.204.nip.io
    ingressClassName: nginx
  persistence:
    enabled: true
    storageClassName: default
prometheus:
  ingress:
    enabled: true
    hosts:
    - prometheus.20.249.129.204.nip.io
    ingressClassName: nginx
  prometheusSpec:
    retention: 10d
    retentionSize: ""

```





#### [참고] 기본 설정 가져오기

만약 특정 설정이 너무 오래되어 확인할 수 없는 경우에 다음 명령으로 기본 차트의 값을 가져와서 새로 시작할 수 있다.

```sh
$ helm show values prometheus-community/kube-prometheus-stack > default-values.yaml

```



#### [참고] manifest 상태점검

```sh
$ helm -n monitoring get manifest prometheus-stack

```





### (2) scrape_configs 등록

Prometheus Stack은 Prometheus Operator를 기반으로 동작하며, prometheus.yaml 파일 대신 Operator가 관리하는 **Custom Resource Definition (CRD)**를 사용해 설정을 관리한다. Prometheus Operator는 Prometheus 객체의 spec 필드를 통해 모든 설정을 관리하며, Helm을 사용하여 설정을 조정할 때는 반드시 values.yaml 파일을 사용하여 scrape 설정을 추가해야 한다.



#### configmap 추가

```sh
$ cat <<EOF | kubectl -n monitoring apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-stack-redis-scrape-config
  namespace: monitoring
data:
  redis-scrape-config.yaml: |
    - job_name: 'redis-exporter'
      static_configs:
        - targets:
          - abc-redis-metrics.redis-system.svc:9121
      metrics_path: /metrics
      scrape_interval: 10s
      scrape_timeout: 10s
EOF


```



#### 추가할 값만 별도의 values.yaml 파일로 적용

--reuse-values 옵션을 사용하면, 이전 설정 값은 유지한 채로 새로 추가한 값만 적용할 수 있다.

```sh
$ cat > redis-scrape-values.yaml
---
prometheus:
  prometheusSpec:
    additionalScrapeConfigs:
      - configMap:
          name: prometheus-stack-redis-scrape-config
          key: redis-scrape-config.yaml
---


$ helm -n monitoring upgrade prometheus-stack prometheus-community/kube-prometheus-stack \
    -f redis-scrape-values.yaml \
    --reuse-values



$ helm -n monitoring ls
NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                       APP VERSION
prometheus-stack        monitoring      1               2024-09-09 20:29:59.934899 +0900 KST    deployed        kube-prometheus-stack-62.5.0v0.76.1
prometheus-stack        monitoring      2               2024-09-10 14:05:56.435389 +0900 KST    deployed        kube-prometheus-stack-62.6.0v0.76.1


```





## 7) nodeSelector 지정

해당 POD InfraNode에 스케쥴링되도록 Node Selector를 지정한다.

Deployment와 Statefulset 에 아래와 같이 추가한다.

```sh

apiVersion: apps/v1
kind: Deployment
metadata:
spec:
  template:
    spec:
      containers:
      nodeSelector:         <-- nodeSelector 셋팅한다.
        nodepool: infra     <-- 
...
```





# 3. UI 접속

Helm 차트를 통해 Grafana가 설치되었으므로, 이를 구성하고 접근하는 방법을 설정해야 한다.



## 1) **Grafana/Prometheus UI 접속**

domain 확인

```sh
# Grafana domain 확인
$ kubectl get ingress -n monitoring
NAME                                    CLASS   HOSTS                              ADDRESS   PORTS   AGE
prometheus-stack-grafana                nginx   grafana.20.249.129.204.nip.io                80      25s
prometheus-stack-kube-prom-prometheus   nginx   prometheus.20.249.129.204.nip.io             80      25s


# Grafana password 확인
$ kubectl -n monitoring get secret -n monitoring prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

New1234!




# ID / PASS
# admin / New1234!


```





# 4. Prometheus



## 1) 접속URL

```
http://prometheus.20.249.129.204.nip.io

```



## 2) Target

* Prometheus가 모니터링하고 있는 모든 엔드포인트를 표기함
* Prometheus는 `prometheus.yml` 파일의 `scrape_configs` 섹션에서 설정된 대상들을 스크랩함

* 메뉴
  * Status > Target
* Targets의 주요 기능
  - **모니터링 상태 확인**
    - Prometheus가 대상의 메트릭을 정상적으로 수집하고 있는지 확인
    - 특정 대상이 `UP` 상태인지, `DOWN` 상태인지, 스크랩에 실패한 이유는 무엇인지 등을 파악
  - **문제 해결**
    - 어떤 대상에서 문제가 발생했는지, 스크랩에 실패한 이유는 무엇인지 확인
  - **라벨 및 메트릭 필터링**
    - 각 대상에 적용된 라벨을 통해 메트릭을 필터링하고 구체적인 데이터를 분석

## 3) Graph

* Prometheus에서 저장된 매트릭 데이터를 시각화 하여 보여준다.
* PromQL 을 직접 쿼리하여 결과를 확인할 수 있다.



### PromQL 예제

```sh
# 단순메트릭 조회
up

# 특정 매티릭 필터링
up{job="node-exporter"}
up{job="node-exporter", instance="10.30.16.4:9100"}


# node_memory_MemFree 예제
node_memory_MemFree_bytes

# 필터링
node_memory_MemFree_bytes{instance="10.30.16.4:9100"}

# 5동안의 Rate를 계산하여 보여준다.
rate(node_memory_MemFree_bytes{instance="10.30.16.4:9100"}[5m])

```





# 5. Grafana



## 1) 접속URL

```

# DEV
http://grafana.20.249.129.204.nip.io

# PRD
http://grafana.4.230.144.108.nip.io

```



## 2) CoreDNS

* 메뉴 : Home > Dashboards > 
* CoreDNSCoreDNS는 Kubernetes의 기본 DNS 서버로, 클러스터 내 서비스 디스커버리 및 DNS 이름 해석을 담당
* 클러스터의 안정성과 네트워크 성능을 확인하기 위해 CoreDNS의 성능과 상태를 모니터링 수행

![image-20240607172013854](./.10.PrometheusStack.assets/image-20240607172013854.png)



## 3) Namespace (Pods)

* 메뉴 : Home > Dashboards > Kubernetes / Compute Resources / Namespace (Pods)
* POD 별 리소스(CPU/Memory)의 사용량을 확인 가능
* 특정 POD 를 클릭하면 POD 만 볼수 있는 별도의 모니터링 화면으로 이동됨

![image-20240607170545283](./.10.PrometheusStack.assets/image-20240607170545283.png)





## 4) Namespace (Workloads)

* 메뉴 : Home > Dashboards > Kubernetes / Compute Resources / Namespace (Pods)
* Workloads 별 리소스(CPU/Memory)의 사용량을 확인 가능
* 개별 POD가 아닌 POD 를 배포하는 Workload 단위로 리소스를 확인 함



## 5) Nodes

* 메뉴 : Home > Dashboards > Node Exporter / Nodes
* Node별 CPU, Memory, Disk 사용량을 확인한다.

* 실제 운영 환경에서도 POD 들이 과도하게 Scheduling 될때가 자주 발생함.
* 이때 Node상태를 확인하는 중요한 모니터링이 된다.

